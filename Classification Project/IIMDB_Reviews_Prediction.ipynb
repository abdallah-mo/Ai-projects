{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb14b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"IMDB Reviews Prediction\n",
    "===========================\n",
    "\n",
    "This module provides a simple pipeline for sentiment analysis on the\n",
    "IMDB movie reviews dataset. The original notebook loaded a CSV file,\n",
    "performed basic exploratory analysis, tokenised the review text, built\n",
    "an LSTM-based neural network using Keras and TensorFlow, trained the\n",
    "model and evaluated it on a held\u2011out test set. This module extracts\n",
    "that logic into reusable functions suitable for a standalone Python\n",
    "script or for importing into other projects.\n",
    "\n",
    "The typical workflow is:\n",
    "\n",
    "* Load the data from a CSV file.\n",
    "* Preprocess the reviews: tokenise and pad sequences, encode labels.\n",
    "* Split the data into training and test sets.\n",
    "* Build the LSTM model.\n",
    "* Train the model, optionally using early stopping.\n",
    "* Evaluate the model on the test set and generate a classification report.\n",
    "* Optionally plot a confusion matrix.\n",
    "\n",
    "Example usage from the command line:\n",
    "\n",
    "``\n",
    "python imdb_reviews_prediction.py --data-path ./IMDB\\ Dataset.csv --epochs 5\n",
    "``\n",
    "\n",
    "The script will train a model and output accuracy and classification\n",
    "metrics. If you wish to integrate this into a larger project or unit\n",
    "test the functions, import the functions defined here instead of\n",
    "running the module as a script.\n",
    "\n",
    "Note: This script expects the input CSV to have two columns named\n",
    "``review`` and ``sentiment``. The ``sentiment`` column should contain\n",
    "string labels such as ``\"positive\"`` and ``\"negative\"`` which will be\n",
    "encoded to integers.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def load_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load the IMDB dataset from a CSV file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Path to the CSV file containing the dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A pandas DataFrame with at least ``review`` and ``sentiment`` columns.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The CSV is read with ``engine='python'`` and ``on_bad_lines='skip'`` to\n",
    "    bypass problematic rows. Adjust these parameters as needed for your\n",
    "    dataset.\n",
    "    \"\"\"\n",
    "    logger.debug(\"Loading data from %s\", file_path)\n",
    "    df = pd.read_csv(file_path, engine=\"python\", on_bad_lines=\"skip\")\n",
    "    # Ensure required columns exist\n",
    "    if 'review' not in df.columns or 'sentiment' not in df.columns:\n",
    "        raise ValueError(\n",
    "            \"Input CSV must contain 'review' and 'sentiment' columns\"\n",
    "        )\n",
    "    logger.info(\"Loaded %d records from %s\", len(df), file_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_data(\n",
    "    df: pd.DataFrame,\n",
    "    max_words: int = 10_000,\n",
    "    max_len: int = 100,\n",
    ") -> Tuple[np.ndarray, np.ndarray, Tokenizer, LabelEncoder]:\n",
    "    \"\"\"Tokenise text, pad sequences and encode labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing ``review`` and ``sentiment`` columns.\n",
    "    max_words : int, optional\n",
    "        Maximum number of words to keep in the tokenizer vocabulary. Defaults to 10_000.\n",
    "    max_len : int, optional\n",
    "        Maximum length of the padded sequences. Defaults to 100.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray\n",
    "        Array of padded integer sequences representing the reviews.\n",
    "    y : np.ndarray\n",
    "        Array of encoded sentiment labels (0 or 1 for binary classification).\n",
    "    tokenizer : Tokenizer\n",
    "        Fitted Keras Tokenizer instance.\n",
    "    label_encoder : LabelEncoder\n",
    "        Fitted scikit\u2011learn LabelEncoder instance.\n",
    "    \"\"\"\n",
    "    logger.debug(\"Fitting tokenizer on %d reviews\", len(df))\n",
    "    tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(df['review'].astype(str).tolist())\n",
    "    sequences = tokenizer.texts_to_sequences(df['review'].astype(str).tolist())\n",
    "    logger.debug(\"Converted texts to sequences\")\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(df['sentiment'].astype(str))\n",
    "    logger.debug(\"Encoded sentiment labels\")\n",
    "    return padded_sequences, labels, tokenizer, label_encoder\n",
    "\n",
    "\n",
    "def build_model(\n",
    "    vocab_size: int,\n",
    "    embed_dim: int = 128,\n",
    "    input_length: int = 100,\n",
    "    lstm_units: List[int] | Tuple[int, ...] = (128, 64),\n",
    "    dropout_rate: float = 0.5,\n",
    "    num_classes: int = 1,\n",
    ") -> Sequential:\n",
    "    \"\"\"Construct and compile an LSTM-based Keras model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab_size : int\n",
    "        Size of the vocabulary (``max_words`` from the preprocessing step).\n",
    "    embed_dim : int, optional\n",
    "        Dimension of the embedding layer. Defaults to 128.\n",
    "    input_length : int, optional\n",
    "        Length of input sequences (``max_len``). Defaults to 100.\n",
    "    lstm_units : list of int, optional\n",
    "        Hidden units for each LSTM layer. Defaults to two layers with 128 and 64 units.\n",
    "    dropout_rate : float, optional\n",
    "        Dropout rate applied after each LSTM layer. Defaults to 0.5.\n",
    "    num_classes : int, optional\n",
    "        Number of output classes. Use 1 for binary classification. Defaults to 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Sequential\n",
    "        A compiled Keras model ready for training.\n",
    "    \"\"\"\n",
    "    logger.debug(\n",
    "        \"Building model with vocab_size=%d, embed_dim=%d, input_length=%d\", vocab_size, embed_dim, input_length\n",
    "    )\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=input_length))\n",
    "    for units in lstm_units[:-1]:\n",
    "        # For all but the last LSTM layer, return sequences so the next layer can receive a 3D input\n",
    "        model.add(LSTM(units, return_sequences=True))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    # Final LSTM layer does not return sequences\n",
    "    model.add(LSTM(lstm_units[-1]))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    # Dense output layer\n",
    "    if num_classes == 1:\n",
    "        activation = 'sigmoid'\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        activation = 'softmax'\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    model.add(Dense(num_classes, activation=activation))\n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['accuracy'])\n",
    "    logger.info(\"Model built and compiled\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: Sequential,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_val: np.ndarray,\n",
    "    y_val: np.ndarray,\n",
    "    epochs: int = 5,\n",
    "    batch_size: int = 64,\n",
    "    early_stopping: bool = True,\n",
    ") -> object:\n",
    "    \"\"\"Train the Keras model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Sequential\n",
    "        The compiled Keras model to train.\n",
    "    X_train, y_train : array-like\n",
    "        Training data and labels.\n",
    "    X_val, y_val : array-like\n",
    "        Validation data and labels for early stopping and evaluation.\n",
    "    epochs : int, optional\n",
    "        Number of training epochs. Defaults to 5.\n",
    "    batch_size : int, optional\n",
    "        Batch size for training. Defaults to 64.\n",
    "    early_stopping : bool, optional\n",
    "        Whether to use early stopping callback on validation loss. Defaults to True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    object\n",
    "        A Keras History object containing details of the training process.\n",
    "    \"\"\"\n",
    "    callbacks = []\n",
    "    if early_stopping:\n",
    "        from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "        callbacks.append(EarlyStopping(monitor='val_loss', patience=3, min_delta=1e-4, restore_best_weights=True))\n",
    "    logger.info(\n",
    "        \"Starting training for %d epochs (early_stopping=%s)\", epochs, early_stopping\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "    logger.info(\"Training complete\")\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_model(model: Sequential, X_test: np.ndarray, y_test: np.ndarray) -> float:\n",
    "    \"\"\"Evaluate the model on the test set and return accuracy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Sequential\n",
    "        The trained Keras model.\n",
    "    X_test, y_test : array-like\n",
    "        Test data and labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Accuracy of the model on the test set.\n",
    "    \"\"\"\n",
    "    logger.debug(\"Evaluating model on test data\")\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    logger.info(\"Test accuracy: %.4f\", accuracy)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def predict_and_report(\n",
    "    model: Sequential,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    label_encoder: LabelEncoder,\n",
    ") -> Tuple[np.ndarray, str]:\n",
    "    \"\"\"Generate predictions and classification report.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Sequential\n",
    "        Trained Keras model.\n",
    "    X_test, y_test : array-like\n",
    "        Test data and true labels.\n",
    "    label_encoder : LabelEncoder\n",
    "        Fitted LabelEncoder used for decoding predictions if needed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_pred : np.ndarray\n",
    "        Predicted class labels as integers.\n",
    "    report : str\n",
    "        Text classification report from scikit\u2011learn.\n",
    "    \"\"\"\n",
    "    logger.debug(\"Predicting on test data\")\n",
    "    predictions = model.predict(X_test, verbose=0)\n",
    "    # Flatten predictions and threshold at 0.5 for binary classification\n",
    "    if predictions.ndim == 2 and predictions.shape[1] == 1:\n",
    "        y_pred = np.round(predictions).astype(int).flatten()\n",
    "    else:\n",
    "        y_pred = predictions.argmax(axis=1)\n",
    "    report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
    "    logger.info(\"Generated classification report\")\n",
    "    return y_pred, report\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm: np.ndarray, class_names: List[str]) -> None:\n",
    "    \"\"\"Plot a confusion matrix using seaborn.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cm : np.ndarray\n",
    "        Confusion matrix array (2x2 for binary classification).\n",
    "    class_names : list of str\n",
    "        Names of the classes corresponding to the rows and columns of the matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Displays a heatmap using matplotlib.\n",
    "    \"\"\"\n",
    "    logger.debug(\"Plotting confusion matrix\")\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def parse_args(args: List[str] | None = None) -> argparse.Namespace:\n",
    "    \"\"\"Parse command line arguments for running the script.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args : list of str, optional\n",
    "        Argument list to parse. If None, defaults to sys.argv[1:].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    argparse.Namespace\n",
    "        Parsed arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Train an LSTM model on the IMDB reviews dataset.\")\n",
    "    parser.add_argument(\n",
    "        '--data-path',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Path to the CSV file containing the IMDB dataset (requires 'review' and 'sentiment' columns).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--epochs', type=int, default=5, help=\"Number of training epochs (default: 5).\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--batch-size', type=int, default=64, help=\"Batch size for training (default: 64).\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--max-words',\n",
    "        type=int,\n",
    "        default=10_000,\n",
    "        help=\"Maximum number of words to keep in the vocabulary (default: 10000).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--max-len',\n",
    "        type=int,\n",
    "        default=100,\n",
    "        help=\"Maximum length of padded sequences (default: 100).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--no-early-stopping',\n",
    "        action='store_true',\n",
    "        help=\"Disable early stopping during training.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--plot-cm',\n",
    "        action='store_true',\n",
    "        help=\"Plot the confusion matrix after evaluation.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--log-level', type=str, default='INFO', help=\"Logging level (e.g. DEBUG, INFO, WARNING)\"\n",
    "    )\n",
    "    parsed_args = parser.parse_args(args)\n",
    "    return parsed_args\n",
    "\n",
    "\n",
    "def main(cli_args: argparse.Namespace | None = None) -> None:\n",
    "    \"\"\"Main entry point for running from the command line.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cli_args : argparse.Namespace, optional\n",
    "        Parsed command line arguments. If None, command line arguments are parsed\n",
    "        from ``sys.argv`` automatically.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    if cli_args is None:\n",
    "        cli_args = parse_args()\n",
    "    # Configure logging\n",
    "    logging.basicConfig(level=getattr(logging, cli_args.log_level.upper(), logging.INFO))\n",
    "\n",
    "    # Load and preprocess data\n",
    "    df = load_data(cli_args.data_path)\n",
    "    X, y, tokenizer, label_encoder = preprocess_data(\n",
    "        df, max_words=cli_args.max_words, max_len=cli_args.max_len\n",
    "    )\n",
    "\n",
    "    # Split data into train/validation and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    # Further split the training set into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42\n",
    "    )\n",
    "    logger.info(\n",
    "        \"Data split into %d training samples, %d validation samples and %d test samples\",\n",
    "        len(X_train),\n",
    "        len(X_val),\n",
    "        len(X_test),\n",
    "    )\n",
    "\n",
    "    # Build and train the model\n",
    "    model = build_model(\n",
    "        vocab_size=min(cli_args.max_words, len(tokenizer.word_index) + 1),\n",
    "        input_length=cli_args.max_len,\n",
    "    )\n",
    "    train_model(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_val,\n",
    "        y_val,\n",
    "        epochs=cli_args.epochs,\n",
    "        batch_size=cli_args.batch_size,\n",
    "        early_stopping=not cli_args.no_early_stopping,\n",
    "    )\n",
    "\n",
    "    # Evaluate and report\n",
    "    accuracy = evaluate_model(model, X_test, y_test)\n",
    "    y_pred, report = predict_and_report(model, X_test, y_test, label_encoder)\n",
    "    print(\"\\nTest accuracy: {:.4f}\".format(accuracy))\n",
    "    print(\"\\nClassification report:\\n\")\n",
    "    print(report)\n",
    "\n",
    "    # Optionally plot confusion matrix\n",
    "    if cli_args.plot_cm:\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plot_confusion_matrix(cm, class_names=label_encoder.classes_.tolist())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
